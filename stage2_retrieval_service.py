#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¶æ®µäºŒï¼šæ£€ç´¢æœåŠ¡é˜¶æ®µ
è´Ÿè´£æŸ¥è¯¢å¤„ç†ã€å‘é‡æ£€ç´¢ã€æŒ‡ä»¤ç”Ÿæˆå’Œç»“æœè¿”å›

ä¸»è¦åŠŸèƒ½ï¼š
1. æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„å¥å­å’Œè¯­è¨€å‚æ•°
2. æ‰§è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ï¼ˆå¥å­å’Œå®ä½“ï¼‰
3. ç”Ÿæˆå®Œæ•´çš„NERæŒ‡ä»¤æ¨¡æ¿
4. è¿”å›æ ¼å¼åŒ–çš„æ£€ç´¢ç»“æœ
"""

import os
import sys
import json
import logging
import argparse
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Union

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config import (
    STAGE2_MILVUS_CONFIG,
    STAGE2_RETRIEVAL_CONFIG,
    STAGE2_MODEL_CONFIG,
    STAGE2_OUTPUT_CONFIG,
    LOGGING_CONFIG,
    ENTITY_TYPES,
    SUPPORTED_LANGUAGES,
    INSTRUCTION_TEMPLATE,
    ENTITY_TYPE_FORMAT
)
from database.milvus_client import MilvusClient
from core.embedding_model import EmbeddingModel
from core.retrieval_engine import RetrievalEngine


def setup_logging():
    """
    è®¾ç½®æ—¥å¿—é…ç½®
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = Path(LOGGING_CONFIG["log_file"]).parent
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=getattr(logging, LOGGING_CONFIG["level"]),
        format=LOGGING_CONFIG["format"],
        handlers=[
            logging.StreamHandler(),  # æ§åˆ¶å°è¾“å‡º
            logging.FileHandler(
                LOGGING_CONFIG["log_file"], 
                encoding='utf-8'
            ) if LOGGING_CONFIG["enable_file_logging"] else logging.NullHandler()
        ]
    )
    
    return logging.getLogger(__name__)


class Stage2RetrievalService:
    """
    é˜¶æ®µäºŒï¼šæ£€ç´¢æœåŠ¡å¤„ç†å™¨
    è´Ÿè´£å®Œæ•´çš„æ£€ç´¢å’ŒæŒ‡ä»¤ç”Ÿæˆæµç¨‹
    """
    
    def __init__(self, config_override: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æ£€ç´¢æœåŠ¡å¤„ç†å™¨
        
        Args:
            config_override: é…ç½®è¦†ç›–å‚æ•°
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åˆå¹¶é…ç½®
        self.milvus_config = {**STAGE2_MILVUS_CONFIG}
        self.retrieval_config = {**STAGE2_RETRIEVAL_CONFIG}
        self.model_config = {**STAGE2_MODEL_CONFIG}
        self.output_config = {**STAGE2_OUTPUT_CONFIG}
        
        if config_override:
            self.milvus_config.update(config_override.get('milvus', {}))
            self.retrieval_config.update(config_override.get('retrieval', {}))
            self.model_config.update(config_override.get('model', {}))
            self.output_config.update(config_override.get('output', {}))
        
        self.logger.info("ğŸš€ åˆå§‹åŒ–é˜¶æ®µäºŒï¼šæ£€ç´¢æœåŠ¡å¤„ç†å™¨")
        self.logger.info(f"ğŸ” æ£€ç´¢é…ç½®: Top-Kå®ä½“={self.retrieval_config['top_k_entities']}, Top-Kå¥å­={self.retrieval_config['top_k_sentences']}")
        self.logger.info(f"ğŸ“Š æ•°æ®åº“æ¨¡å¼: {self.milvus_config['mode']}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.milvus_client = None
        self.embedding_model = None
        self.retrieval_engine = None
    
    def initialize_components(self):
        """
        åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        """
        try:
            self.logger.info("ğŸ”§ åˆå§‹åŒ–æ£€ç´¢æœåŠ¡ç»„ä»¶...")
            
            # 1. åˆå§‹åŒ–Milvuså®¢æˆ·ç«¯
            self.logger.info("ğŸ“Š è¿æ¥Milvusæ•°æ®åº“...")
            self.milvus_client = MilvusClient(
                mode=self.milvus_config["mode"],
                local_db_path=self.milvus_config.get("local_db_path"),
                host=self.milvus_config.get("host"),
                port=self.milvus_config.get("port")
            )
            self.logger.info("âœ… Milvusæ•°æ®åº“è¿æ¥æˆåŠŸ")
            
            # 2. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.logger.info("ğŸ¤– åŠ è½½LLM2VecåµŒå…¥æ¨¡å‹...")
            self.embedding_model = EmbeddingModel(
                model_name=self.model_config["model_name"],
                supervised_model=self.model_config.get("supervised_model"),
                pooling_mode=self.model_config["pooling_mode"],
                max_length=self.model_config["max_length"],
                device=self.model_config["device"]
            )
            self.logger.info("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # 3. åˆå§‹åŒ–æ£€ç´¢å¼•æ“
            self.logger.info("âš™ï¸ åˆå§‹åŒ–æ£€ç´¢å¼•æ“...")
            self.retrieval_engine = RetrievalEngine(
                milvus_client=self.milvus_client,
                embedding_model=self.embedding_model
            )
            self.logger.info("âœ… æ£€ç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
            
            self.logger.info("ğŸ‰ æ‰€æœ‰æ£€ç´¢æœåŠ¡ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def validate_input(self, query: str, language: str) -> bool:
        """
        éªŒè¯è¾“å…¥å‚æ•°
        
        Args:
            query: æŸ¥è¯¢å¥å­
            language: è¯­è¨€ä»£ç 
            
        Returns:
            bool: éªŒè¯æ˜¯å¦é€šè¿‡
        """
        try:
            # éªŒè¯æŸ¥è¯¢å¥å­
            if not query or not query.strip():
                self.logger.error("âŒ æŸ¥è¯¢å¥å­ä¸èƒ½ä¸ºç©º")
                return False
            
            # éªŒè¯è¯­è¨€ä»£ç 
            if language not in SUPPORTED_LANGUAGES:
                self.logger.error(f"âŒ ä¸æ”¯æŒçš„è¯­è¨€: {language}ï¼Œæ”¯æŒçš„è¯­è¨€: {SUPPORTED_LANGUAGES}")
                return False
            
            # éªŒè¯å¥å­é•¿åº¦
            if len(query.strip()) > self.model_config["max_length"]:
                self.logger.warning(f"âš ï¸ æŸ¥è¯¢å¥å­é•¿åº¦è¶…è¿‡é™åˆ¶ ({self.model_config['max_length']})ï¼Œå°†è¢«æˆªæ–­")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ è¾“å…¥éªŒè¯å¤±è´¥: {e}")
            return False
    
    def retrieve_similar_sentences(self, query: str, language: str) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ç›¸ä¼¼å¥å­
        
        Args:
            query: æŸ¥è¯¢å¥å­
            language: è¯­è¨€ä»£ç 
            
        Returns:
            List[Dict]: ç›¸ä¼¼å¥å­åˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ” æ£€ç´¢ç›¸ä¼¼å¥å­ (è¯­è¨€: {language}, Top-K: {self.retrieval_config['top_k_sentences']})")
            
            start_time = time.time()
            
            similar_sentences = self.retrieval_engine.retrieve_similar_sentences(
                query=query,
                language=language,
                top_k=self.retrieval_config["top_k_sentences"]
            )
            
            retrieval_time = time.time() - start_time
            
            # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
            threshold = self.retrieval_config["similarity_threshold"]
            if threshold > 0:
                similar_sentences = [
                    sentence for sentence in similar_sentences 
                    if sentence.get("score", 0) >= threshold
                ]
            
            self.logger.info(f"âœ… æ£€ç´¢åˆ° {len(similar_sentences)} ä¸ªç›¸ä¼¼å¥å­ (è€—æ—¶: {retrieval_time:.3f}s)")
            
            return similar_sentences
            
        except Exception as e:
            self.logger.error(f"âŒ ç›¸ä¼¼å¥å­æ£€ç´¢å¤±è´¥: {e}")
            raise
    
    def retrieve_entities_by_types(self, query: str, language: str) -> Dict[str, List[Dict[str, Any]]]:
        """
        æŒ‰ç±»å‹æ£€ç´¢ç›¸å…³å®ä½“
        
        Args:
            query: æŸ¥è¯¢å¥å­
            language: è¯­è¨€ä»£ç 
            
        Returns:
            Dict: æŒ‰å®ä½“ç±»å‹åˆ†ç»„çš„æ£€ç´¢ç»“æœ
        """
        try:
            self.logger.info(f"ğŸ” æ£€ç´¢ç›¸å…³å®ä½“ (è¯­è¨€: {language}, å®ä½“ç±»å‹: {len(ENTITY_TYPES)})")
            
            start_time = time.time()
            
            entity_results = self.retrieval_engine.retrieve_all_entity_types(
                query=query,
                language=language,
                top_k=self.retrieval_config["top_k_entities"]
            )
            
            retrieval_time = time.time() - start_time
            
            # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
            threshold = self.retrieval_config["similarity_threshold"]
            if threshold > 0:
                for entity_type in entity_results:
                    entity_results[entity_type] = [
                        entity for entity in entity_results[entity_type]
                        if entity.get("score", 0) >= threshold
                    ]
            
            # ç»Ÿè®¡æ£€ç´¢ç»“æœ
            total_entities = sum(len(entities) for entities in entity_results.values())
            found_types = len([k for k, v in entity_results.items() if v])
            
            self.logger.info(f"âœ… æ£€ç´¢åˆ° {total_entities} ä¸ªå®ä½“ï¼Œè¦†ç›– {found_types} ä¸ªç±»å‹ (è€—æ—¶: {retrieval_time:.3f}s)")
            
            return entity_results
            
        except Exception as e:
            self.logger.error(f"âŒ å®ä½“æ£€ç´¢å¤±è´¥: {e}")
            raise
    
    def generate_instruction_template(self, query: str, similar_sentences: List[Dict], 
                                    entity_results: Dict[str, List[Dict]]) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„æŒ‡ä»¤æ¨¡æ¿
        
        Args:
            query: æŸ¥è¯¢å¥å­
            similar_sentences: ç›¸ä¼¼å¥å­åˆ—è¡¨
            entity_results: å®ä½“æ£€ç´¢ç»“æœ
            
        Returns:
            str: å®Œæ•´çš„æŒ‡ä»¤æ¨¡æ¿
        """
        try:
            self.logger.info("ğŸ“ ç”ŸæˆNERæŒ‡ä»¤æ¨¡æ¿...")
            
            # 1. ç”Ÿæˆå®ä½“ç±»å‹æ ¼å¼åŒ–æ–‡æœ¬
            entity_type_texts = []
            max_entities_per_type = self.retrieval_config["max_entities_per_type"]
            
            for entity_type in ENTITY_TYPES:
                entities = entity_results.get(entity_type, [])
                entity_examples = [
                    item["entity_text"] for item in entities[:max_entities_per_type]
                ]
                
                if entity_examples:
                    examples_text = ", \n       ".join(entity_examples)
                    entity_type_text = ENTITY_TYPE_FORMAT.format(
                        entity_type=entity_type,
                        examples=examples_text
                    )
                else:
                    entity_type_text = f"- {entity_type}: \n  e.g. (no examples available)"
                
                entity_type_texts.append(entity_type_text)
            
            entity_types_formatted = "\n".join(entity_type_texts)
            
            # 2. ç”Ÿæˆç¤ºä¾‹æ–‡æœ¬
            examples_text = ""
            max_examples = self.retrieval_config["max_examples_in_instruction"]
            
            for i, sentence_info in enumerate(similar_sentences[:max_examples]):
                sentence = sentence_info["sentence_text"]
                ner_labels = sentence_info["ner_labels"]
                
                examples_text += f"Input: {sentence}\n"
                examples_text += f"Output: {ner_labels}\n"
                
                if i < len(similar_sentences) - 1 and i < max_examples - 1:
                    examples_text += "\n"
            
            # 3. å¡«å……æ¨¡æ¿
            instruction = INSTRUCTION_TEMPLATE.format(
                entity_types=entity_types_formatted,
                examples=examples_text
            )
            
            # 4. æ·»åŠ æŸ¥è¯¢å¥å­
            full_instruction = instruction + query
            
            self.logger.info("âœ… æŒ‡ä»¤æ¨¡æ¿ç”Ÿæˆå®Œæˆ")
            
            return full_instruction
            
        except Exception as e:
            self.logger.error(f"âŒ æŒ‡ä»¤æ¨¡æ¿ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def process_single_query(self, query: str, language: str) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæŸ¥è¯¢è¯·æ±‚
        
        Args:
            query: æŸ¥è¯¢å¥å­
            language: è¯­è¨€ä»£ç 
            
        Returns:
            Dict: å®Œæ•´çš„æ£€ç´¢ç»“æœ
        """
        try:
            start_time = time.time()
            
            self.logger.info(f"ğŸ” å¼€å§‹å¤„ç†æŸ¥è¯¢: '{query}' (è¯­è¨€: {language})")
            
            # 1. éªŒè¯è¾“å…¥
            if not self.validate_input(query, language):
                raise ValueError("è¾“å…¥éªŒè¯å¤±è´¥")
            
            # 2. æ£€ç´¢ç›¸ä¼¼å¥å­
            similar_sentences = self.retrieve_similar_sentences(query, language)
            
            # 3. æ£€ç´¢ç›¸å…³å®ä½“
            entity_results = self.retrieve_entities_by_types(query, language)
            
            # 4. ç”ŸæˆæŒ‡ä»¤æ¨¡æ¿
            instruction_template = self.generate_instruction_template(
                query, similar_sentences, entity_results
            )
            
            # 5. æ„å»ºç»“æœ
            processing_time = time.time() - start_time
            
            result = {
                "query": query,
                "language": language,
                "instruction_template": instruction_template,
                "processing_time": processing_time
            }
            
            # æ ¹æ®é…ç½®æ·»åŠ è¯¦ç»†ä¿¡æ¯
            if self.output_config["include_similarity_scores"]:
                result["similar_sentences"] = similar_sentences
                result["entity_results"] = entity_results
            
            if self.output_config["include_statistics"]:
                total_entities = sum(len(entities) for entities in entity_results.values())
                result["statistics"] = {
                    "total_similar_sentences": len(similar_sentences),
                    "total_entities_found": total_entities,
                    "entity_types_found": len([k for k, v in entity_results.items() if v]),
                    "processing_time": processing_time
                }
            
            self.logger.info(f"âœ… æŸ¥è¯¢å¤„ç†å®Œæˆ (è€—æ—¶: {processing_time:.3f}s)")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            raise
    
    def process_jsonl_files(self, jsonl_directories: List[str], input_field: str = 'input', 
                           output_directory: str = 'data/RetriAll_output') -> Dict[str, Any]:
        """
        å¤„ç†JSONLæ–‡ä»¶ï¼Œä¸ºæ¯ä¸ªå¥å­ç”Ÿæˆæ–°çš„æŒ‡ä»¤
        
        Args:
            jsonl_directories: JSONLæ–‡ä»¶ç›®å½•åˆ—è¡¨
            input_field: è¾“å…¥å­—æ®µå
            output_directory: è¾“å‡ºç›®å½•
            
        Returns:
            Dict: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            self.logger.info("ğŸ“‚ å¼€å§‹å¤„ç†JSONLæ–‡ä»¶...")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = Path(output_directory)
            output_path.mkdir(parents=True, exist_ok=True)
            
            processing_stats = {
                'files_processed': 0,
                'total_entries': 0,
                'successful_entries': 0,
                'failed_entries': 0,
                'output_files': []
            }
            
            # å¤„ç†æ¯ä¸ªç›®å½•
            for directory in jsonl_directories:
                dir_path = Path(directory)
                if not dir_path.exists():
                    self.logger.warning(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {directory}")
                    continue
                
                # å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰JSONLæ–‡ä»¶
                for jsonl_file in dir_path.glob("*.jsonl"):
                    self.logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {jsonl_file}")
                    
                    # ç¡®å®šè¯­è¨€
                    language = self._extract_language_from_filename(jsonl_file.name)
                    
                    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
                    output_file = output_path / f"enhanced_{jsonl_file.name}"
                    
                    # å¤„ç†å•ä¸ªJSONLæ–‡ä»¶
                    file_stats = self._process_single_jsonl_file(
                        jsonl_file, output_file, input_field, language
                    )
                    
                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    processing_stats['files_processed'] += 1
                    processing_stats['total_entries'] += file_stats['total_entries']
                    processing_stats['successful_entries'] += file_stats['successful_entries']
                    processing_stats['failed_entries'] += file_stats['failed_entries']
                    processing_stats['output_files'].append(str(output_file))
            
            self.logger.info(f"ğŸ‰ JSONLæ–‡ä»¶å¤„ç†å®Œæˆ: {processing_stats}")
            return processing_stats
            
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†JSONLæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise
    
    def _extract_language_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åä¸­æå–è¯­è¨€ä»£ç """
        language_map = {
            'en': 'en', 'de': 'de', 'es': 'es', 'fr': 'fr',
            'ja': 'ja', 'ko': 'ko', 'ru': 'ru', 'zh': 'zh'
        }
        
        for lang_code in language_map:
            if f"_{lang_code}." in filename or f"_{lang_code}_" in filename:
                return lang_code
        
        return 'en'  # é»˜è®¤è¯­è¨€
    
    def _process_single_jsonl_file(self, input_file: Path, output_file: Path, 
                                  input_field: str, language: str) -> Dict[str, int]:
        """
        å¤„ç†å•ä¸ªJSONLæ–‡ä»¶
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            input_field: è¾“å…¥å­—æ®µå
            language: è¯­è¨€ä»£ç 
            
        Returns:
            Dict: æ–‡ä»¶å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'total_entries': 0,
            'successful_entries': 0,
            'failed_entries': 0
        }
        
        try:
            with open(input_file, 'r', encoding='utf-8') as infile, \
                 open(output_file, 'w', encoding='utf-8') as outfile:
                
                for line_num, line in enumerate(infile, 1):
                    try:
                        # è§£æJSONè¡Œ
                        data = json.loads(line.strip())
                        stats['total_entries'] += 1
                        
                        # è·å–è¾“å…¥æ–‡æœ¬
                        if input_field not in data:
                            self.logger.warning(f"âš ï¸ æ–‡ä»¶ {input_file} ç¬¬ {line_num} è¡Œç¼ºå°‘å­—æ®µ '{input_field}'")
                            stats['failed_entries'] += 1
                            continue
                        
                        query_text = data[input_field]
                        
                        # ç”Ÿæˆæ–°çš„æŒ‡ä»¤
                        result = self.process_single_query(query_text, language)
                        
                        # åˆ›å»ºè¾“å‡ºæ•°æ®
                        output_data = data.copy() if self.output_config.get('preserve_original_fields', True) else {}
                        
                        # æ·»åŠ ç”Ÿæˆçš„æŒ‡ä»¤
                        output_data['enhanced_instruction'] = result.get('instruction_template', '')
                        
                        # å¯é€‰ï¼šæ·»åŠ æ£€ç´¢ä¿¡æ¯
                        if self.output_config.get('include_metadata', True):
                            output_data['retrieval_metadata'] = {
                                'retrieved_entities': result.get('entity_results', {}),
                                'retrieved_sentences': result.get('similar_sentences', []),
                                'language': language,
                                'processing_time': result.get('processing_time', 0)
                            }
                        
                        # å¯é€‰ï¼šæ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°
                        if self.output_config.get('include_similarity_scores', False):
                            output_data['similarity_scores'] = {
                                'entity_scores': {k: [item.get('score', 0) for item in v] 
                                                for k, v in result.get('entity_results', {}).items()},
                                'sentence_scores': [item.get('score', 0) 
                                                  for item in result.get('similar_sentences', [])]
                            }
                        
                        # å†™å…¥è¾“å‡ºæ–‡ä»¶
                        outfile.write(json.dumps(output_data, ensure_ascii=False) + '\n')
                        stats['successful_entries'] += 1
                        
                        # å®šæœŸè®°å½•è¿›åº¦
                        if line_num % 100 == 0:
                            self.logger.info(f"ğŸ“Š å·²å¤„ç† {line_num} è¡Œï¼ŒæˆåŠŸ: {stats['successful_entries']}, å¤±è´¥: {stats['failed_entries']}")
                    
                    except json.JSONDecodeError as e:
                        self.logger.error(f"âŒ æ–‡ä»¶ {input_file} ç¬¬ {line_num} è¡ŒJSONè§£æé”™è¯¯: {e}")
                        stats['failed_entries'] += 1
                    except Exception as e:
                        self.logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {input_file} ç¬¬ {line_num} è¡Œæ—¶å‡ºé”™: {e}")
                        stats['failed_entries'] += 1
            
            self.logger.info(f"âœ… æ–‡ä»¶ {input_file} å¤„ç†å®Œæˆ: {stats}")
            return stats
            
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {input_file} æ—¶å‡ºé”™: {e}")
            raise
    
    def process_batch_queries(self, queries: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†æŸ¥è¯¢è¯·æ±‚
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'query' å’Œ 'language'
            
        Returns:
            List[Dict]: æ‰¹é‡æ£€ç´¢ç»“æœ
        """
        try:
            self.logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç† {len(queries)} ä¸ªæŸ¥è¯¢...")
            
            results = []
            start_time = time.time()
            
            for i, query_info in enumerate(queries):
                query = query_info.get("query", "")
                language = query_info.get("language", "en")
                
                self.logger.info(f"ğŸ“ å¤„ç†æŸ¥è¯¢ {i+1}/{len(queries)}: '{query}' ({language})")
                
                try:
                    result = self.process_single_query(query, language)
                    results.append(result)
                    
                except Exception as e:
                    self.logger.error(f"âŒ æŸ¥è¯¢ {i+1} å¤„ç†å¤±è´¥: {e}")
                    results.append({
                        "query": query,
                        "language": language,
                        "error": str(e),
                        "processing_time": 0
                    })
            
            total_time = time.time() - start_time
            
            self.logger.info(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(queries)} ä¸ªæŸ¥è¯¢ (æ€»è€—æ—¶: {total_time:.3f}s)")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            raise
    
    def save_results(self, results: Union[Dict, List[Dict]], output_file: str):
        """
        ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            results: æ£€ç´¢ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            self.logger.info(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°æ–‡ä»¶: {output_file}")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ç»“æœ
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            self.logger.info("âœ… ç»“æœä¿å­˜å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ ç»“æœä¿å­˜å¤±è´¥: {e}")
            raise
    
    def get_service_statistics(self) -> Dict[str, Any]:
        """
        è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: æœåŠ¡ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            self.logger.info("ğŸ“Š è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯...")
            
            statistics = {
                "service_status": "running",
                "supported_languages": SUPPORTED_LANGUAGES,
                "entity_types": ENTITY_TYPES,
                "configuration": {
                    "top_k_entities": self.retrieval_config["top_k_entities"],
                    "top_k_sentences": self.retrieval_config["top_k_sentences"],
                    "similarity_threshold": self.retrieval_config["similarity_threshold"],
                    "database_mode": self.milvus_config["mode"]
                }
            }
            
            # è·å–æ•°æ®åº“ç»Ÿè®¡
            try:
                database_stats = {}
                for language in SUPPORTED_LANGUAGES:
                    try:
                        entity_collection = f"entity_{language}"
                        sentence_collection = f"sentence_{language}"
                        
                        entity_count = self.milvus_client.get_collection_count(entity_collection)
                        sentence_count = self.milvus_client.get_collection_count(sentence_collection)
                        
                        database_stats[language] = {
                            "entities": entity_count,
                            "sentences": sentence_count
                        }
                        
                    except Exception:
                        database_stats[language] = {
                            "entities": 0,
                            "sentences": 0,
                            "status": "not_available"
                        }
                
                statistics["database_statistics"] = database_stats
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")
                statistics["database_statistics"] = {"error": str(e)}
            
            return statistics
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–æœåŠ¡ç»Ÿè®¡å¤±è´¥: {e}")
            raise


def main():
    """
    å‘½ä»¤è¡Œä¸»ç¨‹åº
    """
    parser = argparse.ArgumentParser(description="é˜¶æ®µäºŒï¼šæ£€ç´¢æœåŠ¡å¤„ç†å™¨")
    parser.add_argument("--query", type=str, help="æŸ¥è¯¢å¥å­")
    parser.add_argument("--language", type=str, default="en", 
                       choices=SUPPORTED_LANGUAGES, help="è¯­è¨€ä»£ç ")
    parser.add_argument("--batch-file", type=str, help="æ‰¹é‡æŸ¥è¯¢æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)")
    parser.add_argument("--jsonl-dirs", nargs="+", help="JSONLæ–‡ä»¶ç›®å½•åˆ—è¡¨")
    parser.add_argument("--input-field", type=str, default="input", help="JSONLæ–‡ä»¶ä¸­çš„è¾“å…¥å­—æ®µå")
    parser.add_argument("--output-file", type=str, help="ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output-dir", type=str, default="data/RetriAll_output", help="JSONLå¤„ç†ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--local-db-path", type=str, help="æœ¬åœ°æ•°æ®åº“è·¯å¾„")
    parser.add_argument("--top-k-entities", type=int, help="æ¯ä¸ªå®ä½“ç±»å‹è¿”å›çš„æ•°é‡")
    parser.add_argument("--top-k-sentences", type=int, help="è¿”å›çš„ç¤ºä¾‹å¥å­æ•°é‡")
    parser.add_argument("--device", choices=["cuda", "cpu"], help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºæœåŠ¡ç»Ÿè®¡ä¿¡æ¯")
    parser.add_argument("--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"], 
                       default="INFO", help="æ—¥å¿—çº§åˆ«")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.log_level:
        LOGGING_CONFIG["level"] = args.log_level
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    
    try:
        # æ„å»ºé…ç½®è¦†ç›–
        config_override = {}
        
        if args.local_db_path:
            config_override["milvus"] = {"local_db_path": args.local_db_path}
        
        if args.top_k_entities or args.top_k_sentences:
            config_override["retrieval"] = {}
            if args.top_k_entities:
                config_override["retrieval"]["top_k_entities"] = args.top_k_entities
            if args.top_k_sentences:
                config_override["retrieval"]["top_k_sentences"] = args.top_k_sentences
        
        if args.device:
            config_override["model"] = {"device": args.device}
        
        if args.output_file or args.output_dir:
            config_override["output"] = {"save_results": True}
            if args.output_dir:
                config_override["output"]["output_directory"] = args.output_dir
        
        # åˆ›å»ºæ£€ç´¢æœåŠ¡å¤„ç†å™¨
        service = Stage2RetrievalService(config_override)
        service.initialize_components()
        
        # å¤„ç†ä¸åŒçš„æ“ä½œæ¨¡å¼
        if args.stats:
            # æ˜¾ç¤ºæœåŠ¡ç»Ÿè®¡ä¿¡æ¯
            statistics = service.get_service_statistics()
            logger.info("ğŸ“Š æœåŠ¡ç»Ÿè®¡ä¿¡æ¯:")
            print(json.dumps(statistics, indent=2, ensure_ascii=False))
            
        elif args.jsonl_dirs:
            # JSONLæ–‡ä»¶å¤„ç†æ¨¡å¼
            logger.info(f"ğŸ“‚ å¤„ç†JSONLç›®å½•: {args.jsonl_dirs}")
            
            results = service.process_jsonl_files(
                jsonl_directories=args.jsonl_dirs,
                input_field=args.input_field,
                output_directory=args.output_dir
            )
            
            logger.info(f"ğŸ‰ JSONLå¤„ç†å®Œæˆ: {results}")
            print(json.dumps(results, indent=2, ensure_ascii=False))
            
        elif args.batch_file:
            # æ‰¹é‡å¤„ç†æ¨¡å¼
            logger.info(f"ğŸ“‚ åŠ è½½æ‰¹é‡æŸ¥è¯¢æ–‡ä»¶: {args.batch_file}")
            
            with open(args.batch_file, 'r', encoding='utf-8') as f:
                queries = json.load(f)
            
            results = service.process_batch_queries(queries)
            
            if args.output_file:
                service.save_results(results, args.output_file)
            else:
                print(json.dumps(results, indent=2, ensure_ascii=False))
            
        elif args.query:
            # å•ä¸ªæŸ¥è¯¢æ¨¡å¼
            result = service.process_single_query(args.query, args.language)
            
            if args.output_file:
                service.save_results(result, args.output_file)
            else:
                print(json.dumps(result, indent=2, ensure_ascii=False))
        
        else:
            # äº¤äº’æ¨¡å¼
            logger.info("ğŸ¯ è¿›å…¥äº¤äº’æ¨¡å¼ï¼Œè¾“å…¥ 'quit' é€€å‡º")
            
            while True:
                try:
                    query = input("\nè¯·è¾“å…¥æŸ¥è¯¢å¥å­: ").strip()
                    if query.lower() in ['quit', 'exit', 'q']:
                        break
                    
                    if not query:
                        continue
                    
                    language = input(f"è¯·è¾“å…¥è¯­è¨€ä»£ç  ({'/'.join(SUPPORTED_LANGUAGES)}) [é»˜è®¤: en]: ").strip()
                    if not language:
                        language = "en"
                    
                    result = service.process_single_query(query, language)
                    
                    print("\n" + "="*80)
                    print("ğŸ¯ ç”Ÿæˆçš„NERæŒ‡ä»¤:")
                    print("="*80)
                    print(result["instruction_template"])
                    print("="*80)
                    
                    if "statistics" in result:
                        stats = result["statistics"]
                        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: ç›¸ä¼¼å¥å­ {stats['total_similar_sentences']} ä¸ª, "
                              f"ç›¸å…³å®ä½“ {stats['total_entities_found']} ä¸ª, "
                              f"è€—æ—¶ {stats['processing_time']:.3f}s")
                    
                except KeyboardInterrupt:
                    break
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
        
        logger.info("âœ… æ£€ç´¢æœåŠ¡å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
    
    finally:
        # æ¸…ç†èµ„æº
        try:
            if 'service' in locals() and service.milvus_client:
                service.milvus_client.close()
                logger.info("ğŸ”’ Milvusè¿æ¥å·²å…³é—­")
        except:
            pass


if __name__ == "__main__":
    main()